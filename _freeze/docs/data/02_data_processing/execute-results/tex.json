{
  "hash": "ccdaf3130f26ee33c218ea71e5f9605c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: data_processing\ntbl-colwidths:\n  - 25\n  - 10\n  - 10\n  - 45\n---\n\n\n\nFor classification tasks, or any categorical data feature we can obtain labels with\n\n::: {.cell execution_count=2}\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n### `get_labels` {.unnumbered}\n> get_labels(df: DataFrame, col_label: str, verbose: bool)\n\n*Extract unique labels from the dataframe and\nsave them to a list. Print the number of labels in the dataset\nas well as the first 5 labels if there are more than five labels \nin the dataset.*\n\nArguments:\n\n|       | type    |default| description|\n|--------|--------|--------|--------|\n| **df** | DataFrame | None | Dataframe in which the labels are contained. |\n| **col_label** | str | None | Name of the column in the dataframe containing labels. |\n| **verbose** | bool | True | Print the statements. Defaults to True. |\n\n:::\n:::\n\n\n``` python\nget_labels(df=my_dataframe, col_label=\"col_label\")\n```\n\n::: {.cell execution_count=3}\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n### `count_tokens` {.unnumbered}\n> count_tokens(df: DataFrame, col_input_ids: str, col_attn_mask: str)\n\n*Counts the number of tokens in each row of a DataFrame where the \nattention mask is 1.*\n\nArguments:\n\n|       | type    |default| description|\n|--------|--------|--------|--------|\n| **df** | DataFrame | None | Dataframe containing the token data. |\n| **col_input_ids** | str | input_ids | Name of the column in df that contains the input IDs. Defaults to \"input_ids\". |\n| **col_attn_mask** | str | None | Name of the column in df that contains the attention masks. Defaults to None. |\n\n:::\n:::\n\n\n# Feature Engineering {.unnumbered}\n\nExtract features from large language models for text classification. \n\n::: {.cell execution_count=4}\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n### `extract_feature_vector` {.unnumbered}\n> extract_feature_vector(data_sample: DatasetDict, model: PreTrainedModel, tokenizer: PreTrainedTokenizerBase, device: str)\n\n*Extract features from large language models for text classification.*\n\nArguments:\n\n|       | type    |default| description|\n|--------|--------|--------|--------|\n| **data_sample** | DatasetDict | None | Dataset including tokenized inputs. Expected to be a dictionary with keys matching the model's expected input names. |\n| **model** | PreTrainedModel | None | The model from which to extract the feature vectors. Should be an instance of a class derived from transformers.PreTrainedModel. |\n| **tokenizer** | PreTrainedTokenizerBase | None | The tokenizer corresponding to the model, used to identify model input names. |\n| **device** | str | None | Compute engine to which the inputs should be transfered. Define using check_device(). |\n\n:::\n:::\n\n\nTokenized dataset means that the `DatasetDict` object has minimally `input_ids` in features for, minimally, `train` split. For some models, like BERT it will also have `attention_mask`. For example:\n\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nmy_dataset\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 125776\n    })\n    validation: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 32342\n    })\n    test: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 21563\n    })\n    other: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 35399\n    })\n    rest: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 1581911\n    })\n})\n```\n:::\n:::\n\n\nExample of usage:\n```python\nmodel_name = \"distilbert-base-uncased\"\ndevice = check_device()\nmodel = AutoModel.from_pretrained(model_name).to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nmy_dataset_lhs = extract_feature_vector(my_dataset, model, tokenizer)\n```\n\n",
    "supporting": [
      "02_data_processing_files/figure-pdf"
    ],
    "filters": []
  }
}