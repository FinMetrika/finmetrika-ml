{
  "hash": "304e516aa1b8868ec6e5e0529faa2da6",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: training\ntbl-colwidths:\n  - 25\n  - 10\n  - 10\n  - 45\n---\n\n\n\n# Training {.unnumbered}\n\n::: {.cell execution_count=2}\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n### `TrainNN` {.unnumbered}\n> TrainNN(model: _empty, training_dataloader: DataLoader, loss_fn: str, optimizer: _empty, num_epochs: int, device: str)\n\n*Train a neural network.*\n\nArguments:\n\n|       | type    |default| description|\n|--------|--------|--------|--------|\n| **model** | _empty | None | Instantiated model class or a defined model architecture. |\n| **training_dataloader** | DataLoader | None | Dataloader for training. |\n| **loss_fn** | str | None | Loss function |\n| **optimizer** | _empty | None | optimizer |\n| **num_epochs** | int | None | Number of epochs to train. |\n| **device** | str | None | Device on which to train the model. Use utils.check_device(). |\n\n:::\n:::\n\n\nLet's see a simple example of randomly generated data:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Define data\nX = np.linspace(0,20, num=200)\ny = X + np.cos(X)*2 + np.random.normal(size=X.shape)\n\n# Create a dataset & dataloader\ndataset_reg = RegressionDataset1D(X,y)\ntraining_dataloader = DataLoader(dataset_reg, shuffle =True)\n```\n:::\n\n\n::: {.cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![](20_training_files/figure-pdf/cell-5-output-1.pdf){}\n:::\n:::\n\n\nLet's fit a simple 2 layer linear model with a `tanh` activation function:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nmodel = nn.Sequential(\n    nn.Linear(1, 10),\n    nn.Tanh(),\n    nn.Linear(10, 1),\n)\n\ntrain = TrainNN(\n    model=model,\n    training_dataloader=training_dataloader,\n    loss_fn=nn.MSELoss(),\n    optimizer=torch.optim.SGD(model.parameters(), lr=0.001),\n    num_epochs=20,\n    device=check_device()\n)\n\n# train the model\nprint(f'Training ... ')\ntrain.train()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUsing mps device!\nTraining ... \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r  0%|          | 0/20 [00:00<?, ?it/s]\r100%|██████████| 20/20 [00:00<00:00, 390167.81it/s]\n```\n:::\n:::\n\n\n# Fine tuning {.unnumbered}\n## Feature extraction {.unnumbered}\n\n::: {.cell execution_count=6}\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n### `FineTuneFtsExtraction` {.unnumbered}\n> FineTuneFtsExtraction(model_name_hf: _empty, dataset_hf: DatasetDict, use_hf: bool)\n\n*Fine tune a model using feature extraction. Training is done on the\nhidden states as features, without modifying the pretrained model.*\n\nArguments:\n\n|       | type    |default| description|\n|--------|--------|--------|--------|\n| **model_name_hf** | _empty | None | Model name as shown on HuggingFace |\n| **dataset_hf** | DatasetDict | None | Dataset dictionary with minimal splits: |\n| **use_hf** | bool | True | Use transformers library for training. |\n\n:::\n:::\n\n\n# Describing the model architecture {.unnumbered}\n\n::: {.cell execution_count=7}\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n### `model_size` {.unnumbered}\n> model_size(model: _empty)\n\n*Count the number of parameters in the model*\n\nArguments:\n\n|       | type    |default| description|\n|--------|--------|--------|--------|\n| **model** | _empty | None | Instantiated model class. |\n\n:::\n:::\n\n\n",
    "supporting": [
      "20_training_files/figure-pdf"
    ],
    "filters": []
  }
}